import av
from av.video.reformatter import ColorRange, Colorspace
from packaging import version as packaging_version
import os
from os import path
import math
from tqdm import tqdm
from PIL import Image
import mimetypes
import re
import torch
from concurrent.futures import ThreadPoolExecutor
from fractions import Fraction
import time
# Add torchaudio import for StreamReader
try:
    import torchaudio
    from torchaudio.io import StreamReader
    HAS_TORCHAUDIO = True
except ImportError:
    HAS_TORCHAUDIO = False


# Add video mimetypes that does not exist in mimetypes
mimetypes.add_type("video/x-ms-asf", ".asf")
mimetypes.add_type("video/x-ms-vob", ".vob")
mimetypes.add_type("video/divx", ".divx")
mimetypes.add_type("video/3gpp", ".3gp")
mimetypes.add_type("video/ogg", ".ogg")
mimetypes.add_type("video/3gpp2", ".3g2")
mimetypes.add_type("video/m2ts", ".m2ts")
mimetypes.add_type("video/m2ts", ".m2t")
mimetypes.add_type("video/m2ts", ".mts")
mimetypes.add_type("video/m2ts", ".ts")
mimetypes.add_type("video/vnd.rn-realmedia", ".rm")  # fake
mimetypes.add_type("video/x-flv", ".flv")  # Not defined on Windows
mimetypes.add_type("video/x-matroska", ".mkv")  # May not be defined for some reason


VIDEO_EXTENSIONS = [
    ".mp4", ".m4v", ".mkv", ".mpeg", ".mpg", ".avi", ".wmv", ".mov", ".flv", ".webm",
    ".asf", ".vob", ".divx", ".3gp", ".ogg", ".3g2", ".m2ts", ".ts", ".rm",
]


def list_videos(directory, extensions=VIDEO_EXTENSIONS):
    return sorted(
        os.path.join(directory, f)
        for f in os.listdir(directory)
        if os.path.splitext(f)[-1].lower() in extensions
    )


# Color space values that are not defined in pyav.
# I am not sure about the compatibility of these values with libsws,
# but it seems to work.
ADDITIONAL_COLORSPACE_VALUES = {
    "UNSPECIFIED": 2,
    "SMPTE170M_2": 6,  # BT.601 NTSC. smpte170m is defined as 5 in libsws
    "SMPTE240M_2": 7,  # smpte240m is defined as 5 in libsws
    "BT2020": 9,
}
AV_VERSION_14 = packaging_version.parse(av.__version__).major >= 14
if not AV_VERSION_14:
    for name, value in ADDITIONAL_COLORSPACE_VALUES.items():
        if getattr(Colorspace, "_by_value") and getattr(Colorspace, "_create") and value not in Colorspace._by_value:
            Colorspace._create(name, value)


COLORSPACE_UNSPECIFIED = 2
COLORSPACE_SMPTE170M = 6
COLORSPACE_SMPTE240M = 7
COLORSPACE_BT2020 = 9
KNOWN_COLORSPACES = {Colorspace.ITU601.value, Colorspace.ITU709.value,
                     COLORSPACE_SMPTE170M, COLORSPACE_SMPTE240M, COLORSPACE_BT2020}


if "libx264" in av.codecs_available:
    LIBH264 = "libx264"
elif "libopenh264" in av.codecs_available:
    LIBH264 = "libopenh264"
else:
    LIBH264 = ""


def add_stream_from_template(container, template):
    # wrapper for av >= 14 compatibility
    if AV_VERSION_14:
        return container.add_stream_from_template(template)
    else:
        return container.add_stream(template=template)


def is_bt709(stream):
    return (stream.codec_context.color_primaries == 1 and
            stream.codec_context.color_trc == 1 and
            stream.codec_context.colorspace == 1)


def is_bt601(stream):
    # bt470bg/bt470bg/smpte170m
    return (stream.codec_context.color_primaries == 5 and
            stream.codec_context.color_trc == 6 and
            stream.codec_context.colorspace == 5)


def has_nvenc():
    return ("h264_nvenc" in av.codec.codecs_available and
            "hevc_nvenc" in av.codec.codecs_available)


def get_fps(stream):
    return stream.guessed_rate


def guess_frames(stream, fps=None, start_time=None, end_time=None, container_duration=None):
    fps = fps or get_fps(stream)
    duration = get_duration(stream, container_duration, to_int=False)

    if duration is None:
        # N/A
        return -1

    if start_time is not None and end_time is not None:
        duration = min(end_time, duration) - start_time
    elif start_time is not None:
        duration = max(duration - start_time, 0)
    elif end_time is not None:
        duration = min(end_time, duration)
    else:
        pass

    return math.ceil(duration * fps)


def get_duration(stream, container_duration=None, to_int=True):
    if stream.duration:
        duration = float(stream.duration * stream.time_base)
    else:
        duration = container_duration

    if duration is None:
        # N/A
        return -1

    if to_int:
        return math.ceil(duration)
    else:
        return duration


def get_frames(stream, container_duration=None):
    if stream.frames > 0:
        return stream.frames
    else:
        # frames is unknown
        return guess_frames(stream, container_duration=container_duration)


def from_image(im):
    return av.video.frame.VideoFrame.from_image(im)


def to_tensor(frame, device=None):
    x = torch.from_numpy(frame.to_ndarray(format="rgb24"))
    if device is not None:
        x = x.to(device)
    # CHW float32
    return x.permute(2, 0, 1).contiguous() / 255.0


def from_tensor(x):
    x = (x.permute(1, 2, 0).contiguous() * 255.0).to(torch.uint8).detach().cpu().numpy()
    return av.video.frame.VideoFrame.from_ndarray(x, format="rgb24")


def to_frame(x):
    if torch.is_tensor(x):
        return from_tensor(x)
    else:
        return from_image(x)


def _print_len(stream):
    print("frames", stream.frames)
    print("guessed_frames", guess_frames(stream))
    print("duration", get_duration(stream))
    print("base_rate", float(stream.base_rate))
    print("average_rate", float(stream.average_rate))
    print("guessed_rate", float(stream.guessed_rate))


def convert_known_fps(fps):
    if isinstance(fps, float):
        if fps == 29.97:
            return Fraction(30000, 1001)
        elif fps == 23.976:
            return Fraction(24000, 1001)
        elif fps == 59.94:
            return Fraction(60000, 1001)
        else:
            fps_frac = Fraction(fps)
            fps_frac = fps_frac.limit_denominator(0x7fffffff)
            if fps_frac.denominator > 0x7fffffff or fps_frac.numerator > 0x7fffffff:
                raise ValueError(f"FPS={fps} could not be converted to Fraction={fps_frac}")
            return fps_frac

    return fps


class FixedFPSFilter():
    @staticmethod
    def parse_vf_option(vf):
        video_filters = []
        vf = vf.strip()
        if not vf:
            return video_filters

        for line in re.split(r'(?<!\\),', vf):
            line = line.strip()
            if line:
                col = re.split(r'(?<!\\)=', line, 1)
                if len(col) == 2:
                    filter_name, filter_option = col
                else:
                    filter_name, filter_option = col[0], ""
                filter_name, filter_option = filter_name.strip(), filter_option.strip()
                video_filters.append((filter_name, filter_option))
        return video_filters

    @staticmethod
    def build_graph(graph, template_stream, video_filters):
        buffer = graph.add_buffer(template=template_stream)
        prev_filter = buffer
        for filter_name, filter_option in video_filters:
            new_filter = graph.add(filter_name, filter_option if filter_option else None)
            prev_filter.link_to(new_filter)
            prev_filter = new_filter
        buffersink = graph.add("buffersink")
        prev_filter.link_to(buffersink)
        graph.configure()

    def __init__(self, video_stream, fps, vf="", deny_filters=[], colorspace=None):
        self.graph = av.filter.Graph()
        video_filters = self.parse_vf_option(vf)
        if colorspace is not None:
            video_filters.append(("colorspace", colorspace))
        if fps is not None:
            video_filters.append(("fps", str(fps)))
        video_filters = [(name, option) for name, option in video_filters if name not in deny_filters]
        self.build_graph(self.graph, video_stream, video_filters)

    def update(self, frame):
        self.graph.push(frame)
        try:
            return self.graph.pull()
        except av.error.BlockingIOError:
            return None
        except av.error.EOFError:
            # finished
            return None


class VideoFilter(FixedFPSFilter):
    def __init__(self, video_stream, vf):
        super().__init__(video_stream, fps=None, vf=vf)
        self.dummy = not vf

    def update(self, frame):
        if self.dummy:
            return frame
        else:
            return super().update(frame)


class VideoOutputConfig():
    def __init__(self, pix_fmt="yuv420p", fps=30, options={}, container_options={},
                 output_width=None, output_height=None, colorspace=None,
                 container_format=None,
                 video_codec=None, output_fps=None):
        self.pix_fmt = pix_fmt
        self.fps = fps
        self.output_fps = output_fps
        self.options = options
        self.container_options = container_options
        self.output_width = output_width
        self.output_height = output_height
        if colorspace is not None:
            self.colorspace = colorspace
        else:
            self.colorspace = "unspecified"
        self.container_format = container_format
        self.video_codec = video_codec

        self.state_updated = lambda config: None
        self.state = dict(
            rgb24_options={},
            reformatter=lambda frame: frame,
            source_color_range=None,
            output_colorspace=None,
        )

    def __repr__(self):
        return "VideoOutputConfig({!r})".format(self.__dict__)


def get_default_video_codec(container_format):
    if container_format in {"mp4", "mkv"}:
        return LIBH264
    elif container_format == "avi":
        return "utvideo"
    else:
        raise ValueError(f"Unsupported container format: {container_format}")


def default_config_callback(stream):
    fps = get_fps(stream)
    if float(fps) > 30:
        fps = 30
    return VideoOutputConfig(
        fps=fps,
        options={"preset": "ultrafast", "crf": "20"}
    )


SIZE_SAFE_FILTERS = [
    "fps", "yadif", "bwdif", "nnedi", "w3fdif", "kerndeint",
    "hflip", "vflip",
]


def test_output_size(test_callback, video_stream, vf):
    video_filter = FixedFPSFilter(video_stream, fps=60, vf=vf, deny_filters=SIZE_SAFE_FILTERS)
    empty_image = Image.new("RGB", (video_stream.codec_context.width,
                                    video_stream.codec_context.height), (128, 128, 128))
    test_frame = av.video.frame.VideoFrame.from_image(empty_image).reformat(
        format=video_stream.pix_fmt,
        src_color_range=ColorRange.JPEG, dst_color_range=video_stream.codec_context.color_range)
    pts_step = int((1. / video_stream.time_base) / 30) or 1
    test_frame.pts = pts_step

    try_count = 0
    while True:
        while True:
            frame = video_filter.update(test_frame)
            test_frame.pts = (test_frame.pts + pts_step)
            if frame is not None:
                break
            try_count += 1
            if try_count * video_stream.codec_context.width * video_stream.codec_context.height * 3 > 2000 * 1024 * 1024:
                raise RuntimeError("Unable to estimate output size of video filter")
        output_frame = get_new_frames(test_callback(frame))
        if output_frame:
            output_frame = output_frame[0]
            break
    return output_frame.width, output_frame.height


def get_new_frames(frame_or_frames_or_none):
    if frame_or_frames_or_none is None:
        return []
    elif isinstance(frame_or_frames_or_none, (list, tuple)):
        return frame_or_frames_or_none
    else:
        return [frame_or_frames_or_none]


def parse_time(s):
    try:
        cols = s.split(":")
        if len(cols) == 1:
            return max(int(cols[0], 10), 0)
        elif len(cols) == 2:
            m = int(cols[0], 10)
            s = int(cols[1], 10)
            return max(m * 60 + s, 0)
        elif len(cols) == 3:
            h = int(cols[0], 10)
            m = int(cols[1], 10)
            s = int(cols[2], 10)
            return max(h * 3600 + m * 60 + s, 0)
        else:
            raise ValueError("time must be hh:mm:ss, mm:ss or sec format")
    except ValueError:
        raise ValueError("time must be hh:mm:ss, mm:ss or sec format")


def guess_color_range(input_stream):
    if input_stream.codec_context.color_range in {ColorRange.MPEG.value, ColorRange.JPEG.value}:
        return input_stream.codec_context.color_range
    else:
        if input_stream.pix_fmt.startswith("yuv4"):
            return ColorRange.MPEG
        elif input_stream.pix_fmt.startswith("yuvj4"):
            return ColorRange.JPEG
        elif input_stream.pix_fmt.startswith("rgb") or input_stream.pix_fmt.startswith("gbr"):
            return ColorRange.JPEG
        else:
            return None  # unknown


def guess_colorspace(input_stream):
    if input_stream.codec_context.colorspace != COLORSPACE_UNSPECIFIED:
        return input_stream.codec_context.colorspace
    else:
        # FIXME: maybe old video is BT.601
        if input_stream.height >= 720:
            return Colorspace.ITU709
        else:
            return Colorspace.ITU601


def guess_rgb24_options(input_stream, target_colorspace):
    src_color_range = guess_color_range(input_stream)
    src_colorspace = guess_colorspace(input_stream)

    if src_color_range is not None and src_colorspace is not None:
        if int(target_colorspace) == COLORSPACE_UNSPECIFIED:
            target_colorspace = Colorspace.ITU601
        return dict(
            src_color_range=src_color_range, dst_color_range=ColorRange.JPEG,
            src_colorspace=src_colorspace, dst_colorspace=target_colorspace,
        )
    else:
        return {}


def guess_target_colorspace(input_stream, colorspace_arg, pix_fmt,
                            exported_output_colorspace=None, exported_source_color_range=None):
    colorspace = color_primaries = color_trc = None

    if input_stream is not None and colorspace_arg == "auto":
        colorspace_arg = "copy"
    elif input_stream is None and colorspace_arg in {"auto", "copy"}:
        # image import (generate_video)
        # use exported setting
        if exported_output_colorspace == Colorspace.ITU709.value:
            if exported_source_color_range == ColorRange.MPEG:
                colorspace_arg = "bt709-tv"
            elif exported_source_color_range == ColorRange.JPEG:
                colorspace_arg = "bt709-pc"
            else:
                # unknown
                colorspace_arg = "bt709-tv"
        elif exported_output_colorspace == Colorspace.ITU601.value:
            if exported_source_color_range == ColorRange.MPEG:
                colorspace_arg = "bt601-tv"
            elif exported_source_color_range == ColorRange.JPEG:
                colorspace_arg = "bt601-pc"
            else:
                # unknown
                colorspace_arg = "bt601-tv"
        else:
            # unknown
            colorspace_arg = "bt709-tv"

    if colorspace_arg in {"bt709", "bt709-pc", "bt709-tv"}:
        # bt709
        color_primaries = Colorspace.ITU709
        color_trc = Colorspace.ITU709
        colorspace = Colorspace.ITU709

        if colorspace_arg == "bt709":
            color_range = guess_color_range(input_stream) if input_stream is not None else None
            if color_range is None:
                if exported_source_color_range in {ColorRange.MPEG.value, ColorRange.JPEG.value}:
                    color_range = exported_source_color_range
                else:
                    color_range = ColorRange.MPEG
        elif colorspace_arg == "bt709-tv":
            color_range = ColorRange.MPEG
        elif colorspace_arg == "bt709-pc":
            color_range = ColorRange.JPEG

    elif colorspace_arg in {"bt601", "bt601-pc", "bt601-tv"}:
        # bt470bg/bt470bg/smpte170m
        color_primaries = Colorspace.ITU601
        color_trc = 6
        colorspace = Colorspace.ITU601

        if colorspace_arg == "bt601":
            color_range = guess_color_range(input_stream) if input_stream is not None else None
            if color_range is None:
                if exported_source_color_range in {ColorRange.MPEG.value, ColorRange.JPEG.value}:
                    color_range = exported_source_color_range
                else:
                    color_range = ColorRange.MPEG
        elif colorspace_arg == "bt601-tv":
            color_range = ColorRange.MPEG
        elif colorspace_arg == "bt601-pc":
            color_range = ColorRange.JPEG

    elif colorspace_arg == "copy":
        # copy from source
        # might cause an error if the value is incompatible with h264
        color_primaries = input_stream.codec_context.color_primaries
        color_trc = input_stream.codec_context.color_trc
        colorspace = input_stream.codec_context.colorspace
        color_range = input_stream.codec_context.color_range
        if color_range == ColorRange.UNSPECIFIED.value:
            color_range = guess_color_range(input_stream) if input_stream is not None else None
            if color_range is None:
                if exported_source_color_range in {ColorRange.MPEG.value, ColorRange.JPEG.value}:
                    color_range = exported_source_color_range
                else:
                    color_range = ColorRange.UNSPECIFIED.value

    if color_range == ColorRange.JPEG.value:
        # replace for full range
        if pix_fmt == "yuv420p":
            pix_fmt = "yuvj420p"
        elif pix_fmt == "yuv444p":
            pix_fmt = "yuvj444p"

    return color_primaries, color_trc, colorspace, color_range, pix_fmt


def configure_colorspace(output_stream, input_stream, config):
    assert config.colorspace in {"unspecified", "auto", "copy",
                                 "bt709", "bt709-tv", "bt709-pc",
                                 "bt601", "bt601-tv", "bt601-pc",
                                 "bt2020", "bt2020-tv", "bt2020-pc"}
    config.state["rgb24_options"] = rgb24_options = {}
    config.state["reformatter"] = reformatter = lambda frame: frame
    exported_source_color_range = config.state["source_color_range"]
    exported_output_colorspace = config.state["output_colorspace"]
    if config.pix_fmt in {"rgb24", "gbrp"} or config.colorspace == "unspecified":
        config.state["source_color_range"] = config.state["output_colorspace"] = None
        if config.state_updated:
            config.state_updated(config)
        return

    if output_stream is not None:
        color_primaries, color_trc, colorspace, color_range, pix_fmt = guess_target_colorspace(
            input_stream, config.colorspace, config.pix_fmt,
            exported_output_colorspace, exported_source_color_range,
        )
        config.pix_fmt = pix_fmt  # replace
        output_stream.codec_context.color_primaries = color_primaries
        output_stream.codec_context.color_trc = color_trc
        output_stream.codec_context.colorspace = colorspace
        output_stream.codec_context.color_range = color_range

        if output_stream.codec_context.colorspace in KNOWN_COLORSPACES:
            if input_stream is not None:
                rgb24_options = guess_rgb24_options(
                    input_stream,
                    target_colorspace=output_stream.codec_context.colorspace)
                reformatter_src_colorspace = rgb24_options["dst_colorspace"]  # output_stream.codec_context.colorspace
                reformatter_src_color_range = rgb24_options["dst_color_range"]  # ColorRange.JPEG
            else:
                # image import (generate_video)
                if exported_output_colorspace in KNOWN_COLORSPACES:
                    reformatter_src_colorspace = exported_output_colorspace
                    reformatter_src_color_range = ColorRange.JPEG
                else:
                    # NOTE: export is executed with colorspace=unspecified or rgb24 or old version,
                    #       so the conversion may not be correct
                    reformatter_src_colorspace = output_stream.codec_context.colorspace
                    if exported_source_color_range in {ColorRange.JPEG.value, ColorRange.MPEG.value}:
                        reformatter_src_color_range = exported_source_color_range
                    else:
                        reformatter_src_color_range = output_stream.codec_context.color_range

            reformatter = lambda frame: frame.reformat(
                format=config.pix_fmt,
                src_colorspace=reformatter_src_colorspace,
                dst_colorspace=output_stream.codec_context.colorspace,
                src_color_range=reformatter_src_color_range,
                dst_color_range=output_stream.codec_context.color_range)
        elif output_stream.codec_context.color_range in {ColorRange.MPEG.value, ColorRange.JPEG.value}:
            # colorspace is unspecified, use guessed value

            if input_stream is not None:
                target_colorspace = guess_colorspace(input_stream)
                rgb24_options = guess_rgb24_options(input_stream, target_colorspace=target_colorspace)
                reformatter_src_colorspace = rgb24_options["dst_colorspace"]  # output_stream.codec_context.colorspace
                reformatter_src_color_range = rgb24_options["dst_color_range"]  # ColorRange.JPEG
            else:
                # image import (generate_video)
                if exported_output_colorspace in KNOWN_COLORSPACES:
                    target_colorspace = exported_output_colorspace
                    reformatter_src_colorspace = exported_output_colorspace
                    reformatter_src_color_range = ColorRange.JPEG
                else:
                    # No guess, use ITU709
                    target_colorspace = Colorspace.ITU709
                    reformatter_src_colorspace = Colorspace.ITU709
                    reformatter_src_color_range = output_stream.codec_context.color_range

            reformatter = lambda frame: frame.reformat(
                format=config.pix_fmt,
                src_colorspace=reformatter_src_colorspace, dst_colorspace=target_colorspace,
                src_color_range=reformatter_src_color_range, dst_color_range=output_stream.codec_context.color_range)
    else:
        # hook video
        assert input_stream is not None

        if config.colorspace in {"auto", "copy"}:
            target_colorspace = guess_colorspace(input_stream)
            rgb24_options = guess_rgb24_options(input_stream, target_colorspace=target_colorspace)
        elif config.colorspace in {"bt709", "bt709-pc", "bt709-tv"}:
            rgb24_options = guess_rgb24_options(input_stream, target_colorspace=Colorspace.ITU709.value)
        elif config.colorspace in {"bt601", "bt601-pc", "bt601-tv"}:
            rgb24_options = guess_rgb24_options(input_stream, target_colorspace=Colorspace.ITU601.value)

    config.state["rgb24_options"] = rgb24_options
    config.state["reformatter"] = reformatter
    if rgb24_options:
        config.state["output_colorspace"] = int(rgb24_options["dst_colorspace"])
    if input_stream is not None:
        config.state["source_color_range"] = int(guess_color_range(input_stream))

    if config.state_updated:
        config.state_updated(config)


def configure_video_codec(config):
    if config.video_codec == "utvideo":
        if config.pix_fmt == "rgb24":
            config.pix_fmt = "gbrp"
        # override unsupported colorspace, pc is not supported
        if config.colorspace in {"bt601", "bt601-pc", "bt601-tv"}:
            config.colorspace = "bt601-tv"
        elif config.colorspace in {"bt709", "bt709-pc", "bt709-tv"}:
            config.colorspace = "bt709-tv"
        elif config.colorspace in {"auto", "copy"}:
            config.colorspace = "bt709-tv"

    if config.video_codec == "libx264":
        if config.pix_fmt in {"rgb24", "gbrp"}:
            config.video_codec = "libx264rgb"
            config.pix_fmt = "rgb24"
        else:
            if config.colorspace in {"bt2020", "bt2020-tv", "bt2020-pc"}:
                # TODO: change pix_fmt
                config.video_codec = "libx265"

    if config.video_codec in {"libx265", "h264_nvenc", "hevc_nvenc"}:
        if config.pix_fmt == "rgb24":
            config.pix_fmt = "gbrp"


def try_replace(output_path_tmp, output_path):
    try_count = 4
    while try_count >= 0:
        try:
            os.replace(output_path_tmp, output_path)
            break
        except: # noqa
            time.sleep(2)
            try_count -= 1
            if try_count <= 0:
                raise


def process_video(input_path, output_path,
                  frame_callback,
                  config_callback=default_config_callback,
                  title=None,
                  vf="",
                  stop_event=None, suspend_event=None, tqdm_fn=None,
                  start_time=None, end_time=None,
                  test_callback=None,
                  use_torchaudio=False):
    """
    Add use_torchaudio option to use torchaudio.io.StreamReader for decoding.
    """
    if use_torchaudio and not HAS_TORCHAUDIO:
        raise ImportError("torchaudio is not installed. Please install torchaudio to use this option.")

    if isinstance(start_time, str):
        start_time = parse_time(start_time)
    if isinstance(end_time, str):
        end_time = parse_time(end_time)
        if start_time is not None and not (start_time < end_time):
            raise ValueError("end_time must be greater than start_time")

    output_path_tmp = path.join(path.dirname(output_path), "_tmp_" + path.basename(output_path))

    if use_torchaudio:
        # --- torchaudio code path ---
        sr = StreamReader(input_path)
        # Find video and audio streams
        video_stream_idx = None
        audio_stream_idx = None
        for i, info in enumerate(sr.get_src_stream_info()):
            if info["media_type"] == "video" and video_stream_idx is None:
                video_stream_idx = i
            elif info["media_type"] == "audio" and audio_stream_idx is None:
                audio_stream_idx = i
        if video_stream_idx is None:
            raise ValueError("No video stream")
        # Add video stream
        sr.add_video_stream(video_stream_idx, frames_per_chunk=1, format="rgb24")
        # Add audio stream if present
        if audio_stream_idx is not None:
            sr.add_audio_stream(audio_stream_idx, frames_per_chunk=1024)
        # Get video stream info
        video_info = sr.get_src_stream_info()[video_stream_idx]
        width = video_info["width"]
        height = video_info["height"]
        fps = video_info["frame_rate"]
        # Setup config
        class DummyStream:
            # Minimal interface for config_callback
            def __init__(self, width, height, fps):
                self.codec_context = type("ctx", (), {})()
                self.codec_context.width = width
                self.codec_context.height = height
                self.guessed_rate = fps
                self.pix_fmt = "rgb24"
        dummy_stream = DummyStream(width, height, fps)
        config = config_callback(dummy_stream)
        config.fps = convert_known_fps(config.fps)
        config.output_fps = convert_known_fps(config.output_fps)
        if not config.container_format:
            config.container_format = path.splitext(output_path)[-1].lower()[1:]
        if not config.video_codec:
            config.video_codec = get_default_video_codec(config.container_format)
        configure_video_codec(config)
        output_container = av.open(output_path_tmp, 'w', options=config.container_options)
        output_size = (config.output_width or width, config.output_height or height)
        output_fps = config.output_fps or config.fps
        video_output_stream = output_container.add_stream(config.video_codec, output_fps)
        configure_colorspace(video_output_stream, None, config)
        video_output_stream.thread_type = "AUTO"
        video_output_stream.pix_fmt = config.pix_fmt
        video_output_stream.width = output_size[0]
        video_output_stream.height = output_size[1]
        video_output_stream.options = config.options
        reformatter = config.state["reformatter"]
        desc = (title if title else input_path)
        ncols = len(desc) + 60
        tqdm_fn = tqdm_fn or tqdm
        # Estimate total frames
        total = None
        try:
            total = int(float(video_info["num_frames"]))
        except Exception:
            pass
        pbar = tqdm_fn(desc=desc, total=total, ncols=ncols)
        # Iterate over video frames
        for (video_chunk, audio_chunk) in sr.stream():
            if video_chunk is not None:
                # video_chunk: (frames, H, W, C), uint8
                for frame_nd in video_chunk:
                    # Convert to torch tensor, then PIL Image, then av.VideoFrame
                    frame = torch.from_numpy(frame_nd.numpy()) if hasattr(frame_nd, 'numpy') else torch.tensor(frame_nd)
                    # (H, W, C) -> (C, H, W)
                    frame = frame.permute(2, 0, 1).contiguous() / 255.0
                    # Convert to av.VideoFrame
                    av_frame = from_tensor(frame)
                    for new_frame in get_new_frames(frame_callback(av_frame)):
                        new_frame = reformatter(new_frame)
                        enc_packet = video_output_stream.encode(new_frame)
                        if enc_packet:
                            output_container.mux(enc_packet)
                        pbar.update(1)
            if suspend_event is not None:
                suspend_event.wait()
            if stop_event is not None and stop_event.is_set():
                break
        # Flush encoder
        for new_frame in get_new_frames(frame_callback(None)):
            new_frame = reformatter(new_frame)
            enc_packet = video_output_stream.encode(new_frame)
            if enc_packet:
                output_container.mux(enc_packet)
            pbar.update(1)
        packet = video_output_stream.encode(None)
        if packet:
            output_container.mux(packet)
        pbar.close()
        output_container.close()
        if not (stop_event is not None and stop_event.is_set()):
            if path.exists(output_path_tmp):
                try_replace(output_path_tmp, output_path)
        return
    # --- original PyAV code path ---
    input_container = av.open(input_path)
    if input_container.duration:
        container_duration = float(input_container.duration / av.time_base)
    else:
        container_duration = None

    if len(input_container.streams.video) == 0:
        raise ValueError("No video stream")

    if start_time is not None:
        input_container.seek(start_time * av.time_base, backward=True, any_frame=False)

    video_input_stream = input_container.streams.video[0]
    video_input_stream.thread_type = "AUTO"

    # has audio stream
    audio_input_streams = [s for s in input_container.streams.audio]
    subtitle_input_streams = [s for s in input_container.streams.subtitles]
    config = config_callback(video_input_stream)
    config.fps = convert_known_fps(config.fps)
    config.output_fps = convert_known_fps(config.output_fps)

    if not config.container_format:
        config.container_format = path.splitext(output_path)[-1].lower()[1:]
    if not config.video_codec:
        config.video_codec = get_default_video_codec(config.container_format)
    configure_video_codec(config)

    output_container = av.open(output_path_tmp, 'w', options=config.container_options)
    fps_filter = FixedFPSFilter(video_input_stream, fps=config.fps, vf=vf)
    if config.output_width is not None and config.output_height is not None:
        output_size = config.output_width, config.output_height
    else:
        if test_callback is None:
            test_callback = frame_callback
        output_size = test_output_size(test_callback, video_input_stream, vf)

    output_fps = config.output_fps or config.fps
    video_output_stream = output_container.add_stream(config.video_codec, output_fps)
    configure_colorspace(video_output_stream, video_input_stream, config)
    video_output_stream.thread_type = "AUTO"
    video_output_stream.pix_fmt = config.pix_fmt
    video_output_stream.width = output_size[0]
    video_output_stream.height = output_size[1]
    video_output_stream.options = config.options
    rgb24_options = config.state["rgb24_options"]
    reformatter = config.state["reformatter"]
    if video_input_stream.metadata is not None:
        for key, value in video_input_stream.metadata.items():
            video_output_stream.metadata[key] = value

    default_acodec = "aac"
    audio_output_streams = []
    for audio_input_stream in audio_input_streams:
        if audio_input_stream.rate < 16000:
            audio_output_stream = output_container.add_stream(default_acodec, 16000)
            audio_copy = False
        elif start_time is not None:
            audio_output_stream = output_container.add_stream(default_acodec, audio_input_stream.rate)
            audio_copy = False
        else:
            try:
                audio_output_stream = add_stream_from_template(output_container, template=audio_input_stream)
                audio_copy = True
            except ValueError:
                audio_output_stream = output_container.add_stream(default_acodec, audio_input_stream.rate)
                audio_copy = False
        if audio_input_stream.metadata is not None:
            for key, value in audio_input_stream.metadata.items():
                audio_output_stream.metadata[key] = value
        audio_output_streams.append((audio_input_stream, audio_output_stream, audio_copy))

    subtitle_output_streams = []
    for subtitle_input_stream in subtitle_input_streams:
        subtitle_output_stream = add_stream_from_template(output_container, template=subtitle_input_stream)
        if subtitle_input_stream.metadata is not None:
            for key, value in subtitle_input_stream.metadata.items():
                subtitle_output_stream.metadata[key] = value
        subtitle_output_streams.append((subtitle_input_stream, subtitle_output_stream))

    desc = (title if title else input_path)
    ncols = len(desc) + 60
    tqdm_fn = tqdm_fn or tqdm
    total = guess_frames(video_input_stream, output_fps, start_time=start_time, end_time=end_time,
                         container_duration=container_duration)
    pbar = tqdm_fn(desc=desc, total=total, ncols=ncols)
    streams = [video_input_stream] + [s[0] for s in audio_output_streams] + [s[0] for s in subtitle_output_streams]

    ctx = video_input_stream.codec_context
    cuvid_dec_test = False

    if((video_input_stream.codec.name+'_cuvid') in av.codec.codecs_available):
        ctx = av.Codec((video_input_stream.codec.name+'_cuvid'), 'r').create()
        ctx.extradata = video_input_stream.codec_context.extradata
        video_input_stream.thread_type = "AUTO"
        cuvid_dec_test = True
    if(video_input_stream.codec.name == 'libdav1d' and 'av1_cuvid' in av.codec.codecs_available):
        ctx = av.Codec(('av1_cuvid'), 'r').create()
        ctx.extradata = video_input_stream.codec_context.extradata
        video_input_stream.thread_type = "AUTO"
        cuvid_dec_test = True

    for packet in input_container.demux(streams):
        if packet.pts is not None:
            if end_time is not None and packet.stream.type == "video" and end_time < packet.pts * packet.time_base:
                break
        if packet.stream.type == "video":
            if cuvid_dec_test is True:
                try:
                    ctx.decode(packet)
                except:
                    ctx = video_input_stream.codec_context
                finally:
                    cuvid_dec_test = False
            for frame in ctx.decode(packet):
                frame = fps_filter.update(frame)
                if frame is not None:
                    frame = frame.reformat(format="rgb24", **rgb24_options) if rgb24_options else frame
                    for new_frame in get_new_frames(frame_callback(frame)):
                        new_frame = reformatter(new_frame)
                        enc_packet = video_output_stream.encode(new_frame)
                        if enc_packet:
                            output_container.mux(enc_packet)
                        pbar.update(1)
        elif packet.stream.type == "audio":
            for audio_input_stream, audio_output_stream, audio_copy in audio_output_streams:
                if packet.stream == audio_input_stream:
                    if packet.dts is not None:
                        if audio_copy:
                            packet.stream = audio_output_stream
                            output_container.mux(packet)
                        else:
                            for frame in packet.decode():
                                frame.pts = None
                                enc_packet = audio_output_stream.encode(frame)
                                if enc_packet:
                                    output_container.mux(enc_packet)
        elif packet.stream.type == "subtitle":
            for subtitle_input_stream, subtitle_output_stream in subtitle_output_streams:
                if packet.stream == subtitle_input_stream:
                    packet.stream = subtitle_output_stream
                    output_container.mux(packet)

        if suspend_event is not None:
            suspend_event.wait()
        if stop_event is not None and stop_event.is_set():
            break

    while True:
        frame = fps_filter.update(None)
        if frame is not None:
            frame = frame.reformat(format="rgb24", **rgb24_options) if rgb24_options else frame
            for new_frame in get_new_frames(frame_callback(frame)):
                new_frame = reformatter(new_frame)
                enc_packet = video_output_stream.encode(new_frame)
                if enc_packet:
                    output_container.mux(enc_packet)
                pbar.update(1)
        else:
            break

    for new_frame in get_new_frames(frame_callback(None)):
        new_frame = reformatter(new_frame)
        enc_packet = video_output_stream.encode(new_frame)
        if enc_packet:
            output_container.mux(enc_packet)
        pbar.update(1)

    packet = video_output_stream.encode(None)
    if packet:
        output_container.mux(packet)
    pbar.close()
    output_container.close()
    input_container.close()

    if not (stop_event is not None and stop_event.is_set()):
        if path.exists(output_path_tmp):
            try_replace(output_path_tmp, output_path)
